{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2data(file_path: str) -> list:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        file_path (str): absa file path\n",
    "\n",
    "    Returns:\n",
    "        list: every elements contain 2 item —— sentence, targets\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for i in range(0, len(lines), 3):\n",
    "        sentence = lines[i].strip()\n",
    "        targets = lines[i + 1].strip().split(',')\n",
    "\n",
    "        for target in targets:\n",
    "            target = target.strip()\n",
    "            processed_sentence = re.sub(r'\\$T\\$', target, sentence)\n",
    "\n",
    "            data_tuple = (processed_sentence, target)\n",
    "            res.append(data_tuple)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def build_vocab(data_tuples):\n",
    "    counter = Counter()\n",
    "    for sentence, _ in data_tuples:\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        counter.update(tokens)\n",
    "    return build_vocab_from_iterator([counter.keys()], specials=[\"<unk>\", \"<pad>\"])\n",
    "\n",
    "def sentence_to_tensor(sentence, vocab):\n",
    "    tokens = tokenize_sentence(sentence)\n",
    "    token_ids = [vocab[token] for token in tokens]\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VirmarQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(data):\n",
    "    vocab = build_vocab(data)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    vocab_list = vocab.get_itos()\n",
    "    tensor_data = [sentence_to_tensor(sentence, vocab) for sentence, _ in data]\n",
    "    padded_tensor_data = pad_sequence(tensor_data, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    word_sentences = [[vocab_list[idx] for idx in indexed_sentence] for indexed_sentence in padded_tensor_data]\n",
    "    word2vec_model = Word2Vec(sentences=word_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "    vocab_size = len(vocab_list)\n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "    for idx, word in enumerate(vocab_list):\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_weights[idx] = word2vec_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "            # 将权重矩阵转换为 PyTorch 张量\n",
    "    embedding_weights = torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    # 创建 Embedding 层并初始化权重\n",
    "    embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    embedding.weight = nn.Parameter(embedding_weights)\n",
    "\n",
    "    return vocab, vocab_list, padded_tensor_data, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2matrix(idx_data, embedding):\n",
    "    indices = []\n",
    "    for i in idx_data:\n",
    "        indices.append(i)\n",
    "    return embedding(torch.tensor(indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\\\Code\\\\ABSA\\\\data\\\\train.raw\"\n",
    "data = file2data(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_list, idx_data, embedding = handle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9294,  5122,  4952,  5384,    42,  9294, 10538,    25, 13030,    25,\n",
      "        14010,  7927,  8587, 11283, 14124,  9739,  6273,  8879,    42,  9629,\n",
      "        13067,  5682,  6211,  4922, 13981, 15027, 11909,  4922, 11954,    42,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "print(idx_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9294, 9294])\n",
      "torch.Size([2, 100])\n"
     ]
    }
   ],
   "source": [
    "input_indices = torch.tensor([vocab[vocab_list[9294]], vocab[vocab_list[9294]]])\n",
    "print(input_indices)\n",
    "output_vectors = embedding(input_indices)\n",
    "print(output_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 100])\n"
     ]
    }
   ],
   "source": [
    "print(trans2matrix(idx_data[2], embedding).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
